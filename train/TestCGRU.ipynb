{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path \n",
    "\n",
    "sys.path.insert(0, Path(__file__).parent.parent.as_posix())\n",
    "\n",
    "from train.gru_model import Network, NetworkConfig\n",
    "from train.ctokenizer import CTokenizer\n",
    "from train.languages_list import Languages\n",
    "from train.paths import *\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = CTokenizer()\n",
    "\n",
    "config = NetworkConfig(\n",
    "    num_classes=2,\n",
    "    vocab_size=2**15,\n",
    "    hidden_dim=104,\n",
    "    num_layers=3,\n",
    "    bidirectional=True,\n",
    "    num_threads_per_dir=1,\n",
    ")\n",
    "\n",
    "model = torch.nn.DataParallel(Network.from_config(config))\n",
    "model.load_state_dict(torch.load(\"artifacts/gru_binary/model_21_finetune.pth\"))\n",
    "model.module.save_binary(RESOURCES / \"gru_binary.bin\")\n",
    "\n",
    "config.num_classes = len(Languages)\n",
    "\n",
    "model = torch.nn.DataParallel(Network.from_config(config))\n",
    "model.load_state_dict(torch.load(\"artifacts/gru_lang/model_96.pth\"))\n",
    "model.module.save_binary(RESOURCES / \"gru_lang.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"\"\"print(\"Hello, world!\")\"\"\")\n",
    "tokens = torch.tensor(tokens).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  3381,   915, 28553,    14, 16506, 14169,    11,     1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>|print|(\"|Hello|,| world|!\"|)|<s>|"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "\n",
    "hf_tokenizer = ByteLevelBPETokenizer(\n",
    "    \"artifacts/tokenizer-vocab.json\",\n",
    "    \"artifacts/tokenizer-merges.txt\",\n",
    ")\n",
    "\n",
    "for token in tokens[0]:\n",
    "    print(hf_tokenizer.decode([token]), end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0998, -0.0402]], grad_fn=<AddmmBackward0>)\n",
      "fwd tensor([[-0.0521,  0.1173,  0.0880,  0.1825,  0.1432, -0.0323,  0.0496, -0.0226,\n",
      "         -0.0147, -0.1103, -0.0986, -0.0930,  0.1540,  0.0522, -0.0736, -0.1440,\n",
      "          0.1444,  0.0672, -0.0539, -0.0979, -0.0725, -0.0897,  0.0514, -0.0691,\n",
      "         -0.1434,  0.1721, -0.3144, -0.2082, -0.1600, -0.1931,  0.2100, -0.2327,\n",
      "         -0.0457,  0.0550,  0.0815, -0.0715,  0.0967, -0.1749,  0.1869, -0.0899,\n",
      "          0.1615, -0.0024, -0.0892,  0.0106,  0.3297, -0.2068,  0.1825, -0.0269,\n",
      "          0.0135,  0.0174, -0.0346,  0.0984,  0.1088, -0.0501, -0.1720,  0.0742,\n",
      "          0.2533,  0.0292, -0.1161,  0.2979, -0.1643,  0.0375,  0.1767,  0.0354,\n",
      "          0.1340,  0.0391, -0.0158,  0.2632, -0.0447,  0.1109,  0.0991, -0.3012,\n",
      "          0.0722, -0.2183,  0.0507,  0.0738,  0.1661,  0.2915,  0.1574, -0.1335,\n",
      "          0.1466, -0.2246, -0.1645,  0.0461, -0.0933,  0.0005, -0.3062, -0.0875,\n",
      "         -0.0966,  0.0010, -0.1828,  0.1152,  0.0654, -0.1047, -0.2042, -0.0458,\n",
      "         -0.1496, -0.2438, -0.3567, -0.0076, -0.1047,  0.0633, -0.1531,  0.1249,\n",
      "          0.0448,  0.0838, -0.0332,  0.1450, -0.1124, -0.0142, -0.1207,  0.0593]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "bwd tensor([[ 0.0542, -0.1215, -0.0750,  0.0950, -0.1450, -0.1571, -0.1640, -0.0362,\n",
      "         -0.0192,  0.1057,  0.2189, -0.1084, -0.2347, -0.0236, -0.0471, -0.2176,\n",
      "          0.0295,  0.2023,  0.0744, -0.1212,  0.1154, -0.0807,  0.0947,  0.0290,\n",
      "         -0.0504, -0.2532, -0.0603,  0.0026,  0.2209, -0.1786,  0.0301,  0.2516,\n",
      "         -0.0225,  0.1716, -0.2346, -0.2455,  0.0029, -0.0985, -0.0837,  0.1340,\n",
      "          0.1236, -0.0020,  0.0790, -0.2624,  0.1208,  0.1826,  0.0619, -0.1076,\n",
      "         -0.1041,  0.0448, -0.3729,  0.1906, -0.0249,  0.1191, -0.0218, -0.1765,\n",
      "         -0.0965,  0.0817,  0.2025, -0.0095, -0.0654,  0.4720, -0.0150,  0.1220,\n",
      "         -0.0391,  0.0602, -0.2281,  0.1417,  0.1170, -0.1053, -0.0277,  0.1427,\n",
      "         -0.1024, -0.1903, -0.0759, -0.0459, -0.2815, -0.0570, -0.1275,  0.0849,\n",
      "          0.1623,  0.2119, -0.0236,  0.0839, -0.0565,  0.2672, -0.0617,  0.0626,\n",
      "         -0.0285, -0.0098, -0.0014,  0.0330,  0.1158,  0.0531,  0.0792, -0.1203,\n",
      "          0.0985,  0.0774, -0.2031, -0.1812, -0.0276, -0.0171,  0.2101,  0.0225,\n",
      "          0.0030,  0.2877, -0.0133, -0.2180,  0.3397,  0.0397,  0.1135,  0.0299]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, last_state = model(tokens, return_last_state=True)\n",
    "print(output)\n",
    "print(\"fwd\", last_state[:, :model.hidden_dim])\n",
    "print(\"bwd\", last_state[:, model.hidden_dim:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0257,  0.0232,  0.0430, -0.0220, -0.0367, -0.0375,  0.0320, -0.0510,\n",
       "          0.0235, -0.0592,  0.0077,  0.0385,  0.0356, -0.0656, -0.0661, -0.0378,\n",
       "         -0.0281, -0.0052,  0.0009,  0.0004, -0.0273,  0.0572, -0.0593, -0.0613,\n",
       "         -0.0053,  0.0228, -0.0373,  0.0471, -0.0468, -0.0049,  0.0631, -0.0349,\n",
       "         -0.0617, -0.0218, -0.0104,  0.0237,  0.0166,  0.0535,  0.0417,  0.0280,\n",
       "          0.0191, -0.0242,  0.0445,  0.0006,  0.0048,  0.0428, -0.0607,  0.0139,\n",
       "          0.0200, -0.0539,  0.0214,  0.0289,  0.0491,  0.0256, -0.0022,  0.0592,\n",
       "          0.0258, -0.0275, -0.0236,  0.0659,  0.0273,  0.0254,  0.0271,  0.0337,\n",
       "         -0.0350,  0.0327, -0.0402, -0.0540,  0.0045,  0.0554, -0.0483, -0.0086,\n",
       "         -0.0507,  0.0095,  0.0006,  0.0092,  0.0335, -0.0269, -0.0059,  0.0607,\n",
       "         -0.0557, -0.0491, -0.0525, -0.0119, -0.0589, -0.0644, -0.0122, -0.0638,\n",
       "         -0.0537,  0.0579,  0.0463, -0.0498, -0.0618, -0.0339, -0.0589, -0.0373,\n",
       "         -0.0029,  0.0096, -0.0037, -0.0516,  0.0225, -0.0479, -0.0178, -0.0578,\n",
       "          0.0115, -0.0204,  0.0301, -0.0623, -0.0587,  0.0085, -0.0075, -0.0332,\n",
       "         -0.0304, -0.0523,  0.0047, -0.0407,  0.0616, -0.0448,  0.0655,  0.0297,\n",
       "          0.0093, -0.0012, -0.0407, -0.0158, -0.0387, -0.0087,  0.0317,  0.0499,\n",
       "         -0.0097,  0.0405, -0.0431, -0.0581, -0.0393,  0.0302,  0.0135, -0.0559,\n",
       "         -0.0200, -0.0077, -0.0074,  0.0517,  0.0093,  0.0633,  0.0634,  0.0480,\n",
       "         -0.0593, -0.0657, -0.0562, -0.0213,  0.0595, -0.0445,  0.0425,  0.0230,\n",
       "         -0.0523, -0.0665,  0.0205,  0.0169, -0.0032, -0.0541,  0.0211,  0.0106,\n",
       "          0.0020, -0.0646,  0.0166,  0.0283, -0.0355, -0.0087,  0.0230, -0.0523,\n",
       "          0.0103, -0.0060,  0.0400,  0.0287,  0.0264,  0.0296, -0.0643, -0.0024,\n",
       "          0.0615, -0.0594,  0.0586,  0.0243, -0.0567, -0.0488, -0.0257,  0.0392,\n",
       "         -0.0368,  0.0076,  0.0228, -0.0078, -0.0156,  0.0421,  0.0087, -0.0182,\n",
       "          0.0405, -0.0293,  0.0581,  0.0424,  0.0565,  0.0278, -0.0081, -0.0204,\n",
       "         -0.0407, -0.0047,  0.0399,  0.0552, -0.0014, -0.0241, -0.0510, -0.0236,\n",
       "          0.0499, -0.0171, -0.0659,  0.0179, -0.0273,  0.0255,  0.0274,  0.0113,\n",
       "         -0.0365, -0.0282, -0.0210,  0.0619,  0.0136,  0.0384, -0.0157, -0.0180],\n",
       "        [-0.0138,  0.0308,  0.0450,  0.0635,  0.0437,  0.0096, -0.0394,  0.0612,\n",
       "          0.0578, -0.0094, -0.0273,  0.0576, -0.0081,  0.0048, -0.0316,  0.0178,\n",
       "         -0.0388, -0.0659,  0.0402, -0.0646,  0.0295,  0.0191, -0.0514,  0.0343,\n",
       "         -0.0028,  0.0125, -0.0270, -0.0287,  0.0592,  0.0494, -0.0226,  0.0185,\n",
       "         -0.0565, -0.0348, -0.0175, -0.0206,  0.0578,  0.0569,  0.0094,  0.0116,\n",
       "         -0.0301,  0.0416,  0.0085,  0.0391, -0.0334,  0.0034,  0.0193, -0.0575,\n",
       "         -0.0391, -0.0665,  0.0278,  0.0210, -0.0667,  0.0647, -0.0076,  0.0459,\n",
       "         -0.0297, -0.0574,  0.0084, -0.0021, -0.0417, -0.0387, -0.0391, -0.0265,\n",
       "          0.0323, -0.0133,  0.0067, -0.0035, -0.0155, -0.0130, -0.0342, -0.0320,\n",
       "          0.0014,  0.0227, -0.0337, -0.0475, -0.0341, -0.0276, -0.0518, -0.0564,\n",
       "          0.0378,  0.0544,  0.0350, -0.0624, -0.0437, -0.0654,  0.0365,  0.0009,\n",
       "         -0.0181, -0.0194, -0.0558, -0.0199,  0.0312,  0.0412,  0.0380,  0.0015,\n",
       "         -0.0124,  0.0542, -0.0455,  0.0644, -0.0569,  0.0354, -0.0032,  0.0233,\n",
       "          0.0589, -0.0178, -0.0360,  0.0081,  0.0634, -0.0118,  0.0060,  0.0628,\n",
       "          0.0091, -0.0213,  0.0557, -0.0021,  0.0258,  0.0195,  0.0160, -0.0502,\n",
       "         -0.0157,  0.0411, -0.0204, -0.0109,  0.0509,  0.0638, -0.0515, -0.0143,\n",
       "          0.0177,  0.0403, -0.0661,  0.0544,  0.0057, -0.0370, -0.0300, -0.0299,\n",
       "         -0.0627,  0.0006, -0.0442,  0.0393, -0.0523, -0.0425,  0.0614,  0.0171,\n",
       "         -0.0114,  0.0207, -0.0483,  0.0354, -0.0450,  0.0033,  0.0053, -0.0273,\n",
       "         -0.0147, -0.0025,  0.0243,  0.0149,  0.0356,  0.0615,  0.0438,  0.0256,\n",
       "         -0.0272, -0.0055, -0.0234,  0.0591,  0.0508,  0.0178, -0.0647, -0.0222,\n",
       "          0.0029,  0.0123, -0.0407,  0.0193,  0.0478,  0.0207,  0.0610,  0.0507,\n",
       "         -0.0056,  0.0311, -0.0422, -0.0436, -0.0499,  0.0508,  0.0088,  0.0242,\n",
       "         -0.0237,  0.0325,  0.0288, -0.0197,  0.0206, -0.0296, -0.0143,  0.0046,\n",
       "          0.0022, -0.0248,  0.0366, -0.0357,  0.0402,  0.0139,  0.0531, -0.0289,\n",
       "          0.0081,  0.0352, -0.0165, -0.0033,  0.0423,  0.0247,  0.0007,  0.0243,\n",
       "          0.0168, -0.0607, -0.0575,  0.0328, -0.0414, -0.0359, -0.0458, -0.0178,\n",
       "          0.0006,  0.0079,  0.0396, -0.0439, -0.0335, -0.0537,  0.0259, -0.0232]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0465, 0.0134], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is_miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1291, -0.3364, -0.3660,  ..., -0.3569, -0.3137, -0.6388],\n",
       "        [ 0.2120,  0.2684,  0.2017,  ...,  0.5242,  0.2220,  0.0875],\n",
       "        [-1.5297,  0.1071,  0.6413,  ...,  1.0974,  0.5629, -0.5461],\n",
       "        ...,\n",
       "        [ 0.9364, -0.2255,  0.2171,  ..., -0.1986, -0.3074, -0.7767],\n",
       "        [ 1.8654,  0.0984, -0.4431,  ...,  0.3219, -0.0388, -1.0945],\n",
       "        [ 0.1277,  1.0642, -0.0931,  ...,  0.2538,  0.7706,  0.2253]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight @ model.gru.weight_ih_l0.T + model.gru.bias_ih_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0917, -0.0995,  0.0908,  ..., -0.0321, -0.0317, -0.0810],\n",
       "        [-0.0270, -0.0973,  0.0935,  ..., -0.0920,  0.0526, -0.0094],\n",
       "        [ 0.0572, -0.0755, -0.0741,  ..., -0.0977,  0.0070, -0.0191],\n",
       "        ...,\n",
       "        [-0.0700, -0.0014,  0.0142,  ..., -0.0347, -0.0350, -0.0148],\n",
       "        [ 0.0755,  0.0200,  0.0106,  ..., -0.0166, -0.0224,  0.0874],\n",
       "        [-0.0251, -0.0255, -0.0518,  ..., -0.0632,  0.0015,  0.0604]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gru.weight_hh_l0_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 96])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(model.hidden_dim)\n",
    "\n",
    "num_tokens = len(tokens[0])\n",
    "hiddens = torch.zeros(num_tokens, model.hidden_dim * 2)\n",
    "\n",
    "for i, token_id in enumerate(tokens[0]):\n",
    "    emb = model.embedding.weight[token_id]\n",
    "    rzn_i = emb @ model.gru.weight_ih_l0.T + model.gru.bias_ih_l0\n",
    "    rzn = model.gru.weight_hh_l0 @ h + model.gru.bias_hh_l0\n",
    "    rz = torch.sigmoid(rzn[:-96] + rzn_i[:-96])\n",
    "\n",
    "    n_i = rzn_i[-96:]\n",
    "    n = rzn[-96:]\n",
    "    r = rz[:96]\n",
    "    z = rz[96:]\n",
    "\n",
    "    n = torch.tanh(n_i + r * n)\n",
    "\n",
    "    h = (1 - z) * n + z * h\n",
    "    hiddens[i, :96] = h\n",
    "\n",
    "\n",
    "h = torch.zeros(model.hidden_dim)\n",
    "\n",
    "for i, token_id in enumerate(tokens[0].flip(0)):\n",
    "    emb = model.embedding.weight[token_id]\n",
    "    rzn_i = emb @ model.gru.weight_ih_l0_reverse.T + model.gru.bias_ih_l0_reverse\n",
    "    rzn = model.gru.weight_hh_l0_reverse @ h + model.gru.bias_hh_l0_reverse\n",
    "    rz = torch.sigmoid(rzn[:-96] + rzn_i[:-96])\n",
    "\n",
    "    n_i = rzn_i[-96:]\n",
    "    n = rzn[-96:]\n",
    "    r = rz[:96]\n",
    "    z = rz[96:]\n",
    "\n",
    "    n = torch.tanh(n_i + r * n)\n",
    "\n",
    "    h = (1 - z) * n + z * h\n",
    "    hiddens[num_tokens - i - 1, 96:] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288, 96])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gru.weight_hh_l0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288, 96])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gru.weight_hh_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(model.hidden_dim)\n",
    "\n",
    "num_tokens = len(tokens[0])\n",
    "hiddens2 = torch.zeros(num_tokens, model.hidden_dim * 2)\n",
    "\n",
    "for i, token_id in enumerate(tokens[0]):\n",
    "    emb = hiddens[i]\n",
    "    rzn_i = emb @ model.gru.weight_ih_l1.T + model.gru.bias_ih_l1\n",
    "    rzn = model.gru.weight_hh_l1 @ h + model.gru.bias_hh_l1\n",
    "    rz = torch.sigmoid(rzn[:-96] + rzn_i[:-96])\n",
    "\n",
    "    n_i = rzn_i[-96:]\n",
    "    n = rzn[-96:]\n",
    "    r = rz[:96]\n",
    "    z = rz[96:]\n",
    "\n",
    "    n = torch.tanh(n_i + r * n)\n",
    "\n",
    "    h = (1 - z) * n + z * h\n",
    "    hiddens[i, :96] = h\n",
    "\n",
    "\n",
    "h = torch.zeros(model.hidden_dim)\n",
    "\n",
    "for i, token_id in enumerate(tokens[0].flip(0)):\n",
    "    emb = hiddens[num_tokens - i - 1]\n",
    "    rzn_i = emb @ model.gru.weight_ih_l1_reverse.T + model.gru.bias_ih_l1_reverse\n",
    "    rzn = model.gru.weight_hh_l1_reverse @ h + model.gru.bias_hh_l1_reverse\n",
    "    rz = torch.sigmoid(rzn[:-96] + rzn_i[:-96])\n",
    "\n",
    "    n_i = rzn_i[-96:]\n",
    "    n = rzn[-96:]\n",
    "    r = rz[:96]\n",
    "    z = rz[96:]\n",
    "\n",
    "    n = torch.tanh(n_i + r * n)\n",
    "\n",
    "    h = (1 - z) * n + z * h\n",
    "    hiddens[num_tokens - i - 1, 96:] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2674e-03,  1.1547e-01, -2.6268e-02, -1.6644e-01,  4.5153e-01,\n",
       "        -1.8983e-02,  3.9017e-01, -5.3695e-01, -9.9300e-02,  2.2561e-01,\n",
       "         1.0204e-01, -1.2183e-02, -9.4804e-02, -3.2954e-01,  4.8671e-03,\n",
       "         4.4939e-01, -1.4898e-01, -1.9818e-01,  4.2070e-01,  2.0015e-01,\n",
       "        -2.3484e-01,  6.0277e-02,  2.8386e-01, -2.2724e-01,  2.6578e-02,\n",
       "        -3.1051e-01,  1.4110e-01,  1.3717e-01,  3.6472e-01,  1.2488e-01,\n",
       "         2.0230e-01,  6.4643e-02,  3.8817e-01,  1.6721e-01, -1.3280e-01,\n",
       "         1.9689e-01,  6.0524e-01,  1.5176e-01,  9.8684e-02,  1.0289e-01,\n",
       "         3.8060e-01, -1.6276e-01,  2.7155e-01, -4.0174e-02, -2.8130e-01,\n",
       "        -2.5302e-01, -2.8708e-01, -8.2621e-02,  1.5804e-01,  3.6840e-01,\n",
       "         1.5071e-02, -9.2558e-02, -4.2890e-01,  3.2219e-01,  1.3373e-01,\n",
       "        -2.7282e-02,  2.6250e-01,  6.6655e-02, -2.9119e-01,  2.1725e-01,\n",
       "         2.9385e-01,  1.3017e-01, -2.0773e-01, -3.5081e-01, -3.2323e-01,\n",
       "         3.4531e-01,  6.4780e-02, -2.8133e-01, -3.2618e-01,  6.8492e-02,\n",
       "         4.0036e-01,  2.1921e-01,  1.5368e-01,  1.4413e-01,  2.4504e-01,\n",
       "        -9.1828e-02,  4.2146e-01,  1.3746e-01,  2.4517e-01, -5.0589e-02,\n",
       "         2.9284e-01,  4.7950e-03, -5.7931e-01,  2.6749e-02, -4.5141e-02,\n",
       "         1.3187e-01, -1.1976e-01, -1.8840e-01,  2.9341e-01, -1.6921e-01,\n",
       "         2.9507e-01, -2.6793e-01,  3.1290e-01, -1.1839e-01,  5.6209e-01,\n",
       "        -3.3016e-01, -2.9800e-01,  5.2316e-01, -2.4825e-01,  5.7122e-01,\n",
       "         3.2771e-02,  2.9828e-01,  5.4323e-02, -1.9188e-01,  4.9338e-01,\n",
       "         9.2803e-02, -1.1790e-01,  3.0834e-01, -3.9602e-01,  2.3382e-01,\n",
       "         4.2385e-01, -3.9731e-01,  4.8064e-01, -1.1448e-01,  4.4446e-02,\n",
       "         2.3822e-01,  2.9945e-04, -1.0433e-01,  1.2431e-01,  1.7580e-01,\n",
       "         4.1780e-01,  1.2600e-01, -2.0727e-01, -9.2369e-03, -2.4332e-01,\n",
       "        -1.8726e-02,  1.6786e-01,  3.9559e-01,  2.3603e-01,  2.8904e-01,\n",
       "         3.7627e-01,  3.9310e-01, -4.4353e-01,  7.8615e-02,  2.4793e-01,\n",
       "         1.7217e-01,  4.8835e-01, -3.4491e-02,  1.4677e-01,  1.8479e-01,\n",
       "        -2.6822e-01, -4.4739e-01,  1.3198e-01,  3.2541e-02, -1.5730e-01,\n",
       "        -2.8168e-01,  2.5847e-01, -3.7595e-01, -4.8608e-01, -6.4082e-01,\n",
       "         1.3050e-02,  3.7701e-01,  1.6649e-01,  3.2510e-01, -3.6805e-01,\n",
       "        -2.9982e-01, -5.4876e-03,  6.3282e-01,  2.4751e-01, -1.4497e-01,\n",
       "         2.1448e-01,  4.0739e-01, -1.0350e-02,  1.1643e-01, -1.2307e-01,\n",
       "        -5.3763e-02, -2.5408e-02,  5.6872e-01,  4.2368e-02, -2.1775e-01,\n",
       "        -4.6479e-01, -1.0389e-02, -1.5563e-01,  4.7121e-01, -4.3579e-01,\n",
       "        -1.2787e-01, -1.4838e-01,  3.4448e-01, -7.8752e-02,  6.5819e-03,\n",
       "         4.0929e-01,  1.9709e-01,  2.2065e-01,  5.6241e-02,  6.8470e-01,\n",
       "        -1.2167e-01, -4.7511e-01,  1.1397e-01, -2.6868e-02, -1.2458e-01,\n",
       "         1.1344e-01,  2.5180e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens[0, :192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0822,  0.0340, -0.0178,  ...,  0.0821,  0.0175, -0.0217],\n",
       "        [ 0.0665,  0.0608,  0.0528,  ..., -0.0407, -0.0972, -0.0914],\n",
       "        [-0.0785, -0.0196,  0.0125,  ...,  0.0728,  0.0491,  0.0823],\n",
       "        ...,\n",
       "        [ 0.0404, -0.0208, -0.0552,  ...,  0.0334,  0.0633, -0.0039],\n",
       "        [ 0.0181, -0.0097,  0.0381,  ..., -0.1003,  0.0772,  0.0852],\n",
       "        [ 0.0514,  0.0665,  0.0215,  ...,  0.0080, -0.0827, -0.0342]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gru.weight_ih_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4439, -0.3690,  0.2728,  0.0814, -0.1744,  0.3272, -0.1627, -0.2458,\n",
       "        -0.4925,  0.2791,  0.1791,  0.0408,  0.3220,  0.1953, -0.4881,  0.0455,\n",
       "        -0.0693, -0.0260, -0.0188, -0.6429,  0.0057, -0.7511,  0.1778, -0.1433,\n",
       "        -0.3337,  0.1596, -0.5848,  0.2208, -0.2546,  0.5237, -0.0496, -0.4356,\n",
       "        -0.2800, -0.2929, -0.0741,  0.1820, -0.1036,  0.3627,  0.2627,  0.0113,\n",
       "        -0.1498, -0.1868,  0.2371,  0.1681,  0.0979, -0.2144,  0.0585, -0.2772,\n",
       "         0.6467, -0.1484,  0.4218, -0.1408,  0.2172,  0.4462,  0.0389,  0.2326,\n",
       "        -0.0037, -0.1862, -0.5302,  0.0680, -0.0361, -0.0545,  0.0352, -0.2929,\n",
       "         0.0556, -0.5602, -0.2234, -0.2770,  0.0971,  0.0328,  0.1536,  0.3885,\n",
       "        -0.3169, -0.3771, -0.3718,  0.0122, -0.1149,  0.4573,  0.2858,  0.4390,\n",
       "        -0.3344,  0.4033, -0.6830, -0.4258, -0.1805,  0.2471,  0.0666,  0.0385,\n",
       "         0.0009,  0.4797, -0.4189, -0.1463, -0.4965,  0.6975, -0.3550, -0.2612],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0215, -0.0358]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 0.0853, -0.2138,  0.0700,  0.0861,  0.1821,  0.0767, -0.0189,  0.0239,\n",
       "           0.1247, -0.1619,  0.0156,  0.1476,  0.0509,  0.0982, -0.1010, -0.4197,\n",
       "          -0.0149,  0.0022, -0.0483, -0.2523,  0.1215, -0.2176, -0.1859,  0.0850,\n",
       "           0.0071,  0.1054, -0.0871, -0.0641,  0.1244, -0.1336, -0.2311, -0.0982,\n",
       "          -0.2187, -0.0351,  0.1400,  0.1996,  0.0321, -0.1520, -0.0656,  0.0163,\n",
       "           0.0580,  0.2081,  0.1261, -0.0688,  0.0139, -0.1371,  0.3132, -0.0830,\n",
       "           0.0467,  0.0559, -0.0798, -0.2197, -0.0547, -0.0137, -0.3674, -0.0544,\n",
       "           0.1473, -0.1371, -0.1658, -0.2117, -0.1386, -0.0592,  0.0114, -0.1037,\n",
       "          -0.0268,  0.0589,  0.2894, -0.1416,  0.3079, -0.0819, -0.0044,  0.1828,\n",
       "           0.2914, -0.1299,  0.0364,  0.0801,  0.2095,  0.0148, -0.1327,  0.0351,\n",
       "           0.1733, -0.0445,  0.1198,  0.0740, -0.1330,  0.2385, -0.0835, -0.2088,\n",
       "          -0.1380, -0.1084,  0.0216,  0.0367,  0.0228,  0.2197,  0.0877,  0.1497,\n",
       "          -0.1113,  0.0497, -0.2523, -0.2018,  0.0409,  0.2220,  0.0386,  0.1879,\n",
       "          -0.0251,  0.1092, -0.0267, -0.1940, -0.1651, -0.0312, -0.2419,  0.0316,\n",
       "           0.0336, -0.3284,  0.0832, -0.1001,  0.0863,  0.0790, -0.0209,  0.2824,\n",
       "          -0.3306,  0.1321, -0.1181,  0.2381, -0.2160,  0.2207, -0.0815,  0.0814,\n",
       "           0.3606,  0.2841, -0.2072, -0.0990, -0.0063,  0.0482, -0.1761, -0.3275,\n",
       "          -0.1663,  0.2404, -0.1030,  0.2163, -0.2099,  0.0723,  0.0289,  0.0162,\n",
       "          -0.0485,  0.2994,  0.1078,  0.2459,  0.1331, -0.2450,  0.1572,  0.1065,\n",
       "          -0.1286, -0.0053,  0.3164, -0.1923,  0.0660, -0.1773,  0.0720, -0.1548,\n",
       "          -0.0157, -0.0033,  0.1023,  0.0289, -0.1005, -0.0234,  0.3456,  0.3003,\n",
       "           0.1366,  0.0597,  0.1087,  0.1217,  0.0154, -0.1265,  0.1676, -0.2922,\n",
       "          -0.0244,  0.1048,  0.1431, -0.2747,  0.0290, -0.1197, -0.0868,  0.2097,\n",
       "          -0.0162,  0.0474,  0.3221,  0.2860, -0.0049, -0.0792,  0.3388, -0.2071]],\n",
       "        grad_fn=<CatBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
